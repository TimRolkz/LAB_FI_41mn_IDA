{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15eb48dd",
   "metadata": {},
   "source": [
    "# 1.  Завдання щодо генерації текстів або машинного перекладу (на вибір) на базі рекурентних мереж або трансформерів (на вибір). \n",
    "# Вирішіть завдання щодо генерації текстів або машинного перекладу. Особливо вітаються україномовні моделі.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeec1f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "data_path = \"ukr.txt\"  \n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "english_texts = []\n",
    "ukrainian_texts = []\n",
    "for line in lines[:10000]:  \n",
    "    eng = line.strip().split('\\t')[0]\n",
    "    ukr = line.strip().split('\\t')[1]\n",
    "    english_texts.append(eng)\n",
    "    ukrainian_texts.append('\\t' + ukr + '\\n')  \n",
    "eng_tokenizer = Tokenizer()\n",
    "ukr_tokenizer = Tokenizer(char_level=True)\n",
    "eng_tokenizer.fit_on_texts(english_texts)\n",
    "ukr_tokenizer.fit_on_texts(ukrainian_texts)\n",
    "\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "ukr_vocab_size = len(ukr_tokenizer.word_index) + 1\n",
    "\n",
    "encoder_input_data = pad_sequences(eng_tokenizer.texts_to_sequences(english_texts), padding='post')\n",
    "decoder_input_data = pad_sequences(ukr_tokenizer.texts_to_sequences(ukrainian_texts), padding='post')\n",
    "decoder_target_data = np.zeros_like(decoder_input_data)\n",
    "decoder_target_data[:, :-1] = decoder_input_data[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45cb174d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 109ms/step - loss: 1.3567 - val_loss: 0.7259\n",
      "Epoch 2/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 119ms/step - loss: 0.6294 - val_loss: 0.6266\n",
      "Epoch 3/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 105ms/step - loss: 0.5586 - val_loss: 0.5739\n",
      "Epoch 4/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 102ms/step - loss: 0.5032 - val_loss: 0.5407\n",
      "Epoch 5/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 100ms/step - loss: 0.4647 - val_loss: 0.5154\n",
      "Epoch 6/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 110ms/step - loss: 0.4278 - val_loss: 0.4923\n",
      "Epoch 7/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 112ms/step - loss: 0.3943 - val_loss: 0.4760\n",
      "Epoch 8/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 103ms/step - loss: 0.3671 - val_loss: 0.4592\n",
      "Epoch 9/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 103ms/step - loss: 0.3358 - val_loss: 0.4429\n",
      "Epoch 10/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 95ms/step - loss: 0.3067 - val_loss: 0.4351\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c4504da690>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedding = Embedding(eng_vocab_size, 256)(encoder_inputs)\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "_, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(ukr_vocab_size, 256)(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(ukr_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    validation_split=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b94cd783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_embedding, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "326bcdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sequence(input_seq, encoder_model, decoder_model, ukr_tokenizer):\n",
    "    states = encoder_model.predict(input_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = ukr_tokenizer.word_index['\\t']  \n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = ukr_tokenizer.index_word.get(sampled_token_index, '')\n",
    "        decoded_sentence += sampled_char\n",
    "        if sampled_char == '\\n' or len(decoded_sentence) > 50:\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        states = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cdc03c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вхідне речення (англійське): go\n"
     ]
    }
   ],
   "source": [
    "def decode_sequence(sequence, tokenizer):\n",
    "    reverse_word_index = {v: k for k, v in tokenizer.word_index.items()}\n",
    "    decoded_sentence = ' '.join([reverse_word_index.get(i, '') for i in sequence if i > 0])\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e05544a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вхідне речення (англійське): go\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Перекладене речення (українське): приходь!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sentence = encoder_input_data[0:1]  \n",
    "input_sentence = decode_sequence(encoder_input_data[0], eng_tokenizer)\n",
    "print(\"Вхідне речення (англійське):\", input_sentence)\n",
    "\n",
    "# Переклад\n",
    "translated = translate_sequence(test_sentence, encoder_model, decoder_model, ukr_tokenizer)\n",
    "print(\"Перекладене речення (українське):\", translated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6a3a00",
   "metadata": {},
   "source": [
    "# 2. Проведіть експерименти з моделями бібліотеки Hugging Face (раніше - Hugging Face Transformers, https://huggingface.co/) за допомогою (наприклад) Pipeline модуля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16310ac2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (2023.5.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2fe0068",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Using cached tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from tf-keras) (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (24.12.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.69.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (13.8.1)\n",
      "Requirement already satisfied: namex in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2023.5.7)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\v1415\\appdata\\roaming\\python\\python311\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.2)\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e34873a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\v1415\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\v1415\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love using Hugging Face models!\n",
      "Sentiment: POSITIVE, Score: 0.9993\n",
      "\n",
      "Text: This is the worst experience I've ever had.\n",
      "Sentiment: NEGATIVE, Score: 0.9998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Завантаження класифікатора\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "texts = [\n",
    "    \"I love using Hugging Face models!\",\n",
    "    \"This is the worst experience I've ever had.\"\n",
    "]\n",
    "for text in texts:\n",
    "    result = classifier(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {result[0]['label']}, Score: {result[0]['score']:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b645a640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Згенерований текст:\n",
      "Once upon a time he could hear the voices of his wife, Mrs. Lecourm, and see the bright light of his home light, yet never could he see that the voice of his own wife. That voice, too, knew nothing\n"
     ]
    }
   ],
   "source": [
    "# Завантаження генеративної моделі\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "seed_text = \"Once upon a time\"\n",
    "generated_text = generator(seed_text, max_length=50, num_return_sequences=1)\n",
    "print(\"Згенерований текст:\")\n",
    "for text in generated_text:\n",
    "    print(text[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "113c8d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-base and revision 686f1db (https://huggingface.co/google-t5/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Переклад на французьку:\n",
      "Hugging Face facilite l'utilisation de la NLP.\n"
     ]
    }
   ],
   "source": [
    "# Завантаження моделі перекладу\n",
    "translator = pipeline(\"translation_en_to_fr\")  \n",
    "text = \"Hugging Face makes NLP easy to use.\"\n",
    "translation = translator(text, max_length=50)\n",
    "print(\"Переклад на французьку:\")\n",
    "print(translation[0]['translation_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3af6ce5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\v1415\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\pipelines\\token_classification.py:168: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Іменовані сутності:\n",
      "Entity: ORG, Text: Hugging Face, Score: 0.9200\n",
      "Entity: LOC, Text: New York City, Score: 0.9993\n"
     ]
    }
   ],
   "source": [
    "# Завантаження моделі NER\n",
    "ner_pipeline = pipeline(\"ner\", grouped_entities=True)\n",
    "text = \"Hugging Face was founded in 2016 in New York City.\"\n",
    "entities = ner_pipeline(text)\n",
    "print(\"Іменовані сутності:\")\n",
    "for entity in entities:\n",
    "    print(f\"Entity: {entity['entity_group']}, Text: {entity['word']}, Score: {entity['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69afcd52",
   "metadata": {},
   "source": [
    "# 3. Завдання щодо генерації або стилізації зображень (на вибір)\n",
    "# Вирішіть завдання перенесення стилю або генерації зображень (архітектура за вашим вибором: GAN/DCGAN/VAE/Diffusion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4313ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Налаштування\n",
    "image_size = 28  \n",
    "batch_size = 64\n",
    "timesteps = 1000  \n",
    "learning_rate = 1e-4\n",
    "epochs = 40\n",
    "\n",
    "os.makedirs(\"fashion_mnist_results\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d6c2aced",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)) \n",
    "])\n",
    "\n",
    "dataset = datasets.FashionMNIST(root='fashion_mnist_data', train=True, transform=transform, download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6bfd72a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Лінійна шкала бета-параметрів\n",
    "def linear_beta_schedule(timesteps):\n",
    "    beta_start = 1e-4\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "betas = linear_beta_schedule(timesteps)\n",
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = torch.cat([torch.tensor([1.0]), alphas_cumprod[:-1]])\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
    "posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
    "\n",
    "def add_noise(x, t, noise):\n",
    "    sqrt_alpha_t = sqrt_alphas_cumprod[t].to(device)\n",
    "    sqrt_one_minus_alpha_t = sqrt_one_minus_alphas_cumprod[t].to(device)\n",
    "    return sqrt_alpha_t[:, None, None, None] * x + sqrt_one_minus_alpha_t[:, None, None, None] * noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fb6ecff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.middle = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.middle(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1adceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40] Batch 0/938 Loss: 0.9988\n",
      "Epoch [1/40] Batch 100/938 Loss: 0.2254\n",
      "Epoch [1/40] Batch 200/938 Loss: 0.1945\n",
      "Epoch [1/40] Batch 300/938 Loss: 0.1781\n",
      "Epoch [1/40] Batch 400/938 Loss: 0.1308\n",
      "Epoch [1/40] Batch 500/938 Loss: 0.1279\n",
      "Epoch [1/40] Batch 600/938 Loss: 0.1331\n",
      "Epoch [1/40] Batch 700/938 Loss: 0.0778\n",
      "Epoch [1/40] Batch 800/938 Loss: 0.0942\n",
      "Epoch [1/40] Batch 900/938 Loss: 0.1067\n",
      "Epoch [2/40] Batch 0/938 Loss: 0.1293\n",
      "Epoch [2/40] Batch 100/938 Loss: 0.1125\n",
      "Epoch [2/40] Batch 200/938 Loss: 0.1113\n",
      "Epoch [2/40] Batch 300/938 Loss: 0.1086\n",
      "Epoch [2/40] Batch 400/938 Loss: 0.0678\n",
      "Epoch [2/40] Batch 500/938 Loss: 0.1123\n",
      "Epoch [2/40] Batch 600/938 Loss: 0.1208\n",
      "Epoch [2/40] Batch 700/938 Loss: 0.1078\n",
      "Epoch [2/40] Batch 800/938 Loss: 0.1083\n",
      "Epoch [2/40] Batch 900/938 Loss: 0.1189\n",
      "Epoch [3/40] Batch 0/938 Loss: 0.0957\n",
      "Epoch [3/40] Batch 100/938 Loss: 0.0699\n",
      "Epoch [3/40] Batch 200/938 Loss: 0.1047\n",
      "Epoch [3/40] Batch 300/938 Loss: 0.0870\n",
      "Epoch [3/40] Batch 400/938 Loss: 0.1087\n",
      "Epoch [3/40] Batch 500/938 Loss: 0.1255\n",
      "Epoch [3/40] Batch 600/938 Loss: 0.0848\n",
      "Epoch [3/40] Batch 700/938 Loss: 0.0905\n",
      "Epoch [3/40] Batch 800/938 Loss: 0.1231\n",
      "Epoch [3/40] Batch 900/938 Loss: 0.0999\n",
      "Epoch [4/40] Batch 0/938 Loss: 0.0726\n",
      "Epoch [4/40] Batch 100/938 Loss: 0.1249\n",
      "Epoch [4/40] Batch 200/938 Loss: 0.0937\n",
      "Epoch [4/40] Batch 300/938 Loss: 0.0787\n",
      "Epoch [4/40] Batch 400/938 Loss: 0.0933\n",
      "Epoch [4/40] Batch 500/938 Loss: 0.0872\n",
      "Epoch [4/40] Batch 600/938 Loss: 0.0966\n",
      "Epoch [4/40] Batch 700/938 Loss: 0.0880\n",
      "Epoch [4/40] Batch 800/938 Loss: 0.0815\n"
     ]
    }
   ],
   "source": [
    "model = UNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def save_images(images, path):\n",
    "    images = (images + 1) / 2 \n",
    "    save_image(images, path)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (imgs, _) in enumerate(dataloader):\n",
    "        imgs = imgs.to(device)\n",
    "        batch_size = imgs.size(0)\n",
    "\n",
    "        t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
    "        noise = torch.randn_like(imgs)\n",
    "        noisy_imgs = add_noise(imgs, t, noise)\n",
    "\n",
    "        noise_pred = model(noisy_imgs)\n",
    "        loss = criterion(noise_pred, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] Batch {batch_idx}/{len(dataloader)} Loss: {loss.item():.4f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_noise = torch.randn(16, 1, image_size, image_size).to(device)\n",
    "        test_images = model(test_noise)\n",
    "        save_images(test_images, f\"fashion_mnist_results/epoch_{epoch+1}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca58947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерація зображень\n",
    "model.eval()\n",
    "num_images = 16\n",
    "generated_images = torch.randn(num_images, 1, image_size, image_size).to(device)\n",
    "\n",
    "for t in reversed(range(timesteps)):\n",
    "    z = torch.randn_like(generated_images) if t > 0 else 0\n",
    "    alpha_t = alphas_cumprod[t].to(device)\n",
    "    beta_t = betas[t].to(device)\n",
    "    mean = (1 / torch.sqrt(alpha_t)) * (generated_images - beta_t / torch.sqrt(1 - alpha_t) * model(generated_images))\n",
    "    variance = torch.sqrt(beta_t) * z\n",
    "    generated_images = mean + variance\n",
    "\n",
    "save_images(generated_images, \"fashion_mnist_results/generated.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f329851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
